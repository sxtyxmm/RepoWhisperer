## ✅ Prompt for Project: **Auto-README Generator using DeepSeek-Coder**

---

### 🧾 Project Title:

`repowhisperer`

---

### 📘 Objective:

Create a Python-based system from scratch that uses the **DeepSeek-Coder 33B** open-source LLM to **parse any GitHub project line by line**, understand the architecture **layer by layer** (file/module/class/function level), and **automatically generate a detailed `README.md` file** including:

* Project overview
* Architecture explanation
* File-by-file breakdown
* Key classes/functions and their purpose
* Example usage if available

---

### 🔍 Requirements:

* No use of external readme generators like `readme-md-generator`, `doctoc`, etc.
* No use of OpenAI, proprietary APIs, or prebuilt Copilot-style logic
* Use **only open-source models** — DeepSeek-Coder 33B (preferred) via local inference
* Implement the full code parsing and prompt generation logic yourself
* Everything should run inside **GitHub Codespaces**

---

### 📦 Tech Stack:

* **Language**: Python 3.11+
* **Model**: DeepSeek-Coder 33B Instruct (local via `transformers`)
* **Serving Backend**: Optional (FastAPI/Flask if web UI planned)
* **Model Inference**: `transformers`, `auto-gptq` (quantized) or `vllm` for performance
* **File Parsing**: Custom AST + `os`, `tokenize`, `ast`, `inspect`
* **Output Format**: Markdown (`README.md`)

---

### 🧠 Features:

1. **Line-by-line Parsing**:

   * Parse source code line by line using `ast`, `tokenize`, or regex fallback.
   * Organize data into file > class > function > logic blocks.

2. **Layered Understanding**:

   * Summarize each file's role (entrypoint, module, util, test, etc.)
   * Understand high-level layers (MVC, client-server, REST API structure, etc.)

3. **Prompt Construction**:

   * Build detailed prompts chunk-wise (respect token limits).
   * Include inline code and context to help LLM reason.

4. **Model Query**:

   * Run local inference with DeepSeek-Coder 33B Instruct (quantized preferred for Codespaces)
   * Use streaming/token batching to avoid OOM errors.

5. **README Generation**:

   * Convert LLM response to structured markdown:

     * Introduction
     * Architecture Diagram (ASCII if needed)
     * File Descriptions
     * Key Classes/Functions
     * Sample Usage / How to Run
     * Contribution Guidelines (optional)

6. **CLI Tool**:

   * Add a simple CLI:

     ```bash
     python generate_readme.py --repo ./my_project
     ```

---

### 🛠 Implementation Steps:

1. **Set up Codespace environment** with GPU-enabled image (if supported) or CPU fallback.
2. **Download & load DeepSeek-Coder 33B** with quantization.
3. **Implement file walker**: walk project recursively, ignore `.git`, `venv`, etc.
4. **Extract structure**: use `ast` to extract imports, classes, functions, docstrings.
5. **Generate natural-language summaries** from each code chunk using model.
6. **Merge summaries into README format.**
7. **Write result to `README.md` at project root.**

---

### 📁 Repository Structure:

```bash
auto-readme-generator/
│
├── generate_readme.py       # Main entry point
├── model/
│   └── inference.py         # DeepSeek inference wrapper
├── parser/
│   ├── file_parser.py       # AST-based line/layer parser
│   └── prompt_builder.py    # Prompt construction logic
├── utils/
│   └── markdown_writer.py   # Handles clean markdown formatting
├── README.md                # Generated by the tool
├── requirements.txt
└── config.yaml              # Model config, prompt tuning, exclusions
```

---

### 📎 Example Prompt to LLM (Inside `prompt_builder.py`):

````python
"""
You are a senior full-stack engineer with expertise in AI-assisted developer tools.

Your task is to implement a command-line tool from scratch that:

🔍 INPUT: Accepts the path to a local GitHub repository

🧠 FUNCTION: 
- Walks through the entire project directory
- Parses source code line-by-line and understands the structure layer-by-layer
- Builds a contextual prompt for an open-source code LLM (DeepSeek-Coder 33B-Instruct)
- Sends this prompt to the model running locally and receives a detailed response
- Formats that response into a `README.md` at the root of the project

📦 OUTPUT: A production-grade `README.md` file with the following sections:
1. Project Title
2. Introduction
3. Setup & Installation Instructions
4. Architecture Summary
5. File-by-file Breakdown
6. Key Classes and Functions
7. Example Usage
8. Contribution Guidelines
9. Licensing Information
10. Special Features (if any)
````

Explain what this file does, how it fits in the project, and summarize its key functions.


---

### 🧠 Bonus Ideas:
- Support Mermaid.js diagrams from architecture inference
- Add support for JavaScript, TypeScript using a pluggable parser
- Include test coverage summaries (if test folder present)