## âœ… Prompt for Project: **Auto-README Generator using DeepSeek-Coder**

---

### ğŸ§¾ Project Title:

`repowhisperer`

---

### ğŸ“˜ Objective:

Create a Python-based system from scratch that uses the **DeepSeek-Coder 33B** open-source LLM to **parse any GitHub project line by line**, understand the architecture **layer by layer** (file/module/class/function level), and **automatically generate a detailed `README.md` file** including:

* Project overview
* Architecture explanation
* File-by-file breakdown
* Key classes/functions and their purpose
* Example usage if available

---

### ğŸ” Requirements:

* No use of external readme generators like `readme-md-generator`, `doctoc`, etc.
* No use of OpenAI, proprietary APIs, or prebuilt Copilot-style logic
* Use **only open-source models** â€” DeepSeek-Coder 33B (preferred) via local inference
* Implement the full code parsing and prompt generation logic yourself
* Everything should run inside **GitHub Codespaces**

---

### ğŸ“¦ Tech Stack:

* **Language**: Python 3.11+
* **Model**: DeepSeek-Coder 33B Instruct (local via `transformers`)
* **Serving Backend**: Optional (FastAPI/Flask if web UI planned)
* **Model Inference**: `transformers`, `auto-gptq` (quantized) or `vllm` for performance
* **File Parsing**: Custom AST + `os`, `tokenize`, `ast`, `inspect`
* **Output Format**: Markdown (`README.md`)

---

### ğŸ§  Features:

1. **Line-by-line Parsing**:

   * Parse source code line by line using `ast`, `tokenize`, or regex fallback.
   * Organize data into file > class > function > logic blocks.

2. **Layered Understanding**:

   * Summarize each file's role (entrypoint, module, util, test, etc.)
   * Understand high-level layers (MVC, client-server, REST API structure, etc.)

3. **Prompt Construction**:

   * Build detailed prompts chunk-wise (respect token limits).
   * Include inline code and context to help LLM reason.

4. **Model Query**:

   * Run local inference with DeepSeek-Coder 33B Instruct (quantized preferred for Codespaces)
   * Use streaming/token batching to avoid OOM errors.

5. **README Generation**:

   * Convert LLM response to structured markdown:

     * Introduction
     * Architecture Diagram (ASCII if needed)
     * File Descriptions
     * Key Classes/Functions
     * Sample Usage / How to Run
     * Contribution Guidelines (optional)

6. **CLI Tool**:

   * Add a simple CLI:

     ```bash
     python generate_readme.py --repo ./my_project
     ```

---

### ğŸ›  Implementation Steps:

1. **Set up Codespace environment** with GPU-enabled image (if supported) or CPU fallback.
2. **Download & load DeepSeek-Coder 33B** with quantization.
3. **Implement file walker**: walk project recursively, ignore `.git`, `venv`, etc.
4. **Extract structure**: use `ast` to extract imports, classes, functions, docstrings.
5. **Generate natural-language summaries** from each code chunk using model.
6. **Merge summaries into README format.**
7. **Write result to `README.md` at project root.**

---

### ğŸ“ Repository Structure:

```bash
auto-readme-generator/
â”‚
â”œâ”€â”€ generate_readme.py       # Main entry point
â”œâ”€â”€ model/
â”‚   â””â”€â”€ inference.py         # DeepSeek inference wrapper
â”œâ”€â”€ parser/
â”‚   â”œâ”€â”€ file_parser.py       # AST-based line/layer parser
â”‚   â””â”€â”€ prompt_builder.py    # Prompt construction logic
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ markdown_writer.py   # Handles clean markdown formatting
â”œâ”€â”€ README.md                # Generated by the tool
â”œâ”€â”€ requirements.txt
â””â”€â”€ config.yaml              # Model config, prompt tuning, exclusions
```

---

### ğŸ“ Example Prompt to LLM (Inside `prompt_builder.py`):

````python
"""
You are a senior full-stack engineer with expertise in AI-assisted developer tools.

Your task is to implement a command-line tool from scratch that:

ğŸ” INPUT: Accepts the path to a local GitHub repository

ğŸ§  FUNCTION: 
- Walks through the entire project directory
- Parses source code line-by-line and understands the structure layer-by-layer
- Builds a contextual prompt for an open-source code LLM (DeepSeek-Coder 33B-Instruct)
- Sends this prompt to the model running locally and receives a detailed response
- Formats that response into a `README.md` at the root of the project

ğŸ“¦ OUTPUT: A production-grade `README.md` file with the following sections:
1. Project Title
2. Introduction
3. Setup & Installation Instructions
4. Architecture Summary
5. File-by-file Breakdown
6. Key Classes and Functions
7. Example Usage
8. Contribution Guidelines
9. Licensing Information
10. Special Features (if any)
````

Explain what this file does, how it fits in the project, and summarize its key functions.


---

### ğŸ§  Bonus Ideas:
- Support Mermaid.js diagrams from architecture inference
- Add support for JavaScript, TypeScript using a pluggable parser
- Include test coverage summaries (if test folder present)